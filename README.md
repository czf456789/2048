# 2048

## 采用的算法

ppo。在采取动作之前，优先过滤掉无效的操作，相比于惩罚无效操作，过滤是更好的方法。

30w epcoch震荡。
## 现在的效果
100局 百分之五十达到512 百分之10达到1024
## 奖励函数设置

每轮正奖励：得分/100+空格/16

每局终止状态奖励：-50*（10-log2(每局的最高砖块)）

改进方法：使用蒙特卡洛搜索树方法。尽量避免移动最大数字且最好将其卡在角落



## 产生的问题

### 1.ppo的探索问题

​	随着训练的加深，策略网络已经变为确定性策略，近乎完全失去了探索能力，最终成绩在512~1024震荡。ppo的探索问题如何解决？sac中添加的熵正则可能会更加适合2048这个环境。
